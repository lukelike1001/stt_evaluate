from pathlib import Path
import os
from llm_judge_helper import load_lines_from_file, evaluate_transcript_batch_with_meta_prompting

"""
run_llm_metaprompting.py

Evaluates how an LLM judges reference text with STT text generated by various models.
Here, I'll be using gpt-4o-mini as a judge, and o1-mini for meta-evaluation.
See llm_judge_helper.py for more information on the helper functions were implemented.
"""

# Access the current (src/wer) and parent (src/) directory via Pathlib
curr_dir = Path(__file__).resolve().parent
parent_dir = curr_dir.parent

# Access the transcript paths
ref_path = parent_dir / "transcripts/reference_transcripts.txt"
whisper_base_path = parent_dir / "transcripts" / "whisper_base_transcripts.txt"
whisper_tiny_path = parent_dir / "transcripts" / "whisper_tiny_transcripts.txt"
moonshine_path = parent_dir / "transcripts" / "moonshine_transcripts.txt"

# Then, load the transcripts line-by-line
ref_transcripts = load_lines_from_file(ref_path)
whisper_base_transcripts = load_lines_from_file(whisper_base_path)
whisper_tiny_transcripts = load_lines_from_file(whisper_tiny_path)
moonshine_transcripts = load_lines_from_file(moonshine_path)

# Write output files for all three models that we'll run
whisper_base_output_folder = curr_dir / "metaprompt" / "whisper_base"
whisper_tiny_output_folder = curr_dir / "metaprompt" / "whisper_tiny"
moonshine_output_folder = curr_dir / "metaprompt" / "moonshine"

# Run the evaluation tests for the files with metaprompting this time
evaluate_transcript_batch_with_meta_prompting(ref_transcripts, whisper_base_transcripts, whisper_base_output_folder)
evaluate_transcript_batch_with_meta_prompting(ref_transcripts, whisper_tiny_transcripts, whisper_tiny_output_folder)
evaluate_transcript_batch_with_meta_prompting(ref_transcripts, moonshine_transcripts, moonshine_output_folder)
