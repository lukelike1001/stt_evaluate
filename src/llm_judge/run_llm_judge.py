from pathlib import Path
import os
from llm_judge_helper import load_lines_from_file, evaluate_transcript_batch

"""
run_llm_judge.py

Evaluates how an LLM judges reference text with STT text generated by whisper-base.en
Here, I'll be using gpt-4o-mini. See llm_judge_helper.py for more info
"""

# Access the current (src/wer) and parent (src/) directory via Pathlib
curr_dir = Path(__file__).resolve().parent
parent_dir = curr_dir.parent

# Access the transcript paths
ref_path = parent_dir / "transcripts/reference_transcripts.txt"
whisper_base_path = parent_dir / "transcripts/whisper_base_transcripts.txt"
whisper_tiny_path = parent_dir / "transcripts/whisper_tiny_transcripts.txt"
moonshine_path = parent_dir / "transcripts/moonshine_transcripts.txt"

# Then, load the transcripts line-by-line
ref_transcripts = load_lines_from_file(ref_path)
whisper_base_transcripts = load_lines_from_file(whisper_base_path)
whisper_tiny_transcripts = load_lines_from_file(whisper_tiny_path)
moonshine_transcripts = load_lines_from_file(moonshine_path)

# Write output files for all three models that we'll run
whisper_base_output_folder = curr_dir / "whisper_base"
whisper_tiny_output_folder = curr_dir / "whisper_tiny"
moonshine_output_folder = curr_dir / "moonshine"

# Run the evaluation tests for the files
evaluate_transcript_batch(ref_transcripts, whisper_base_transcripts, whisper_base_output_folder)
evaluate_transcript_batch(ref_transcripts, whisper_tiny_transcripts, whisper_tiny_output_folder)
evaluate_transcript_batch(ref_transcripts, moonshine_transcripts, moonshine_output_folder)
